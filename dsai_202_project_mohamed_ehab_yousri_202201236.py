# -*- coding: utf-8 -*-
"""DSAI_202_Project_Mohamed_Ehab_Yousri_202201236.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tkXeaU889KvEjaNc1qzxuBfFM1GVL6YX

Data Quality Management process
"""

import pandas as pd
import json
import numpy as np

"""1-Data Profiling"""

!pip install dataprofiler

from dataprofiler import Data, Profiler

df=pd.read_csv('/content/heart_disease_uci (New).csv')
df

df=pd.read_csv("/content/heart_disease_uci (New).csv")
from dataprofiler import Data, Profiler                                                        # Profile the dataset
profile = Profiler(df) # Calculate Statistics, Entity Recognition, etc

# Generate a report and use json to prettify.
report  = profile.report(report_options={"output_format":"pretty"})

# Print the report
print(json.dumps(report, indent=4))

# Check for null values
null_values = df.isnull().sum()
null_values

duplicate_rows = df[df.duplicated()]
duplicate_rows

len(duplicate_rows)

df.describe()

"""2-Data Cleaning

Dealing with Duplicates
"""

# Check Data Consistency
# Ensure that the data does not have whitespace or case inconsistencies. You can apply string methods to standardize the data.
df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)

# Duplicate Removal
df_cleaned = df.drop_duplicates(inplace=True)
df_cleaned

data_duplicates = df.duplicated().sum()
print(f"Number of duplicates after cleaning: {data_duplicates}")

"""Dealing With Missing Data"""

# get the number of missing data points per column
missing_values_count = df.isnull().sum()
# look at the # of missing points in the first ten columns
missing_values_count[:]

# how many total missing values do we have?
total_cells = np.product(df.shape)
total_missing = missing_values_count.sum()

# percent of data that is missing
(total_missing/total_cells) * 100

"""Replace the missing values Using Mean, Median, or Mode"""

# Filling missing values for numerical and categorical columns
import pandas as pd
# Assuming 'data' is your DataFrame
numerical_columns = ['trestbps', 'chol', 'thalch', 'oldpeak', 'ca']
categorical_columns = ['fbs', 'restecg', 'exang', 'slope', 'thal']

# For numerical columns, fill with the mean
for col in numerical_columns:
    mean_value = df[col].mean()  # Calculate mean
    df[col].fillna(mean_value, inplace=True)  # Fill missing values with mean

# For categorical columns, fill with the mode (most frequent value)
for col in categorical_columns:
    mode_value = df[col].mode()[0]  # Calculate mode
    df[col].fillna(mode_value, inplace=True)  # Fill missing values with mode

# Check missing values after applying fill
final_missing = df.isnull().sum()
print("Final missing values by column:\n", final_missing)

#  total number of cells in the DataFrame
total_cells = np.product(df.shape)

# total number of missing values
total_missing = final_missing.sum()

# percentage of missing data
percentage_missing = (total_missing / total_cells) * 100
print("Percentage of missing data:", percentage_missing)

"""Dealing with Outliers"""

import matplotlib.pyplot as plt
import seaborn as sns



numerical_columns=['age','trestbps','chol', 'thalch', 'oldpeak','ca']

boxplot = df.boxplot(column=numerical_columns, grid=True, fontsize=9)
plt.ylim(-10, 600)
plt.show()

Q1 = df[numerical_columns].quantile(0.25)
Q3 = df[numerical_columns].quantile(0.75)
IQR = Q3-Q1
print('IQR Scores of columns:\n',IQR)

df

# solving the outlier problem in the "age"column
valid_mean_age = df[df['age'] <= 100]['age'].mean()

# Replace outliers with the calculated mean age inplace
df['age'].where(df['age'] <= 100, valid_mean_age, inplace=True)

# Ensure changes are made by checking the maximum value
max_value = df['age'].max()
print("Maximum value in age:", max_value)

# solving the outlier problem in the "chol"column
valid_mean_chol = df[df['chol'] <= 500]['chol'].mean()

# Replace outliers with the calculated mean chol inplace
df['chol'].where(df['chol'] <= 500, valid_mean_chol, inplace=True)

# Ensure the values of 'chol' column are within the range (0.0 to 500.0)
df['chol'] = df['chol'].clip(lower=0.0, upper=500.0)

# Ensure changes are made by checking the maximum
max_value2 = df['chol'].max()
print("Maximum value in 'chol' column:", max_value2)

# solving the outlier problem in the "thalch"column
Q1 = df['thalch'].quantile(0.25)
Q3 = df['thalch'].quantile(0.75)
IQR = Q3 - Q1

# Define the upper boundary
upper_boundary = Q3 + 1.5 * IQR

# Replace outliers with the upper boundary value
df['thalch'] = df['thalch'].apply(lambda x: upper_boundary if x > upper_boundary else x)
print("Updated maximum value in 'thalch':", df['thalch'].max())

# solving the outlier problem in the "trestbps"column
Q1 = df['trestbps'].quantile(0.25)
Q3 = df['trestbps'].quantile(0.75)
IQR = Q3 - Q1
# Define the lower and upper boundaries
lower_boundary = Q1 - 1.5 * IQR
upper_boundary = Q3 + 1.5 * IQR

# Replace outliers with the boundary values
df['trestbps'] = df['trestbps'].apply(lambda x: lower_boundary if x < lower_boundary else (upper_boundary if x > upper_boundary else x))
print("Updated maximum value in 'trestbps':", df['trestbps'].max())
print("Updated minimum value in 'trestbps':", df['trestbps'].min())

# Set pandas to display all rows
pd.set_option('display.max_rows', None)
print(df)

"""3-Data Validation"""

!pip install pandera

import pandera as pa
from pandera import Column, DataFrameSchema, Check, Index

# Define a schema for the heart disease dataset
heart_disease_schema = pa.DataFrameSchema({
    "age": Column(float, Check.in_range(20.0, 100.0)),  # Assuming age should be between 20.0 and 100.0
    "sex": Column(str, Check.isin(["male", "female"])),  # Assuming only Male and Female categories
    "cp": Column(str),  # More specific checks can be added if the categories are known
    "trestbps": Column(float, Check.in_range(0.0, 200.0)),  # Typical range for blood pressure
    "chol": Column(float, Check.in_range(0.0, 500.0)),  # Assuming cholesterol should be in_range(0.0, 500.0))
    "thal": Column(object),  # Typical range for max heart rate
    "num": Column(int)  # Diagnosis of heart disease
    # Include other columns as needed
})

# Validate the dataset
validated_heart_disease_df = heart_disease_schema(df)
print(validated_heart_disease_df.head())

# # set lazy=True in validate, and use a try-except to catch any SchemaErrors exceptions; you can then pull
#  out the failure cases from the exception into a DataFrame for better readability.
try:
    heart_disease_schema.validate(df, lazy=True)
except pa.errors.SchemaError as exc:
    failure_cases_df = exc.failure_cases
    display(failure_cases_df)

# Schema inference is a function that scans your data and generates a DataFrameSchema with some basic checks;
#  this is intended to provide a starting point for you to tailor and further develop.
!pip install pandera[io]
inferred_schema = pa.infer_schema(df).to_script()
print(inferred_schema)

"""Data Validation Flow with Great Expectations"""

!pip install great_expectations
import great_expectations as gx

!great_expectations init
context = gx.get_context()

validator = df
validator = context.sources.pandas_default.read_csv('/content/heart_disease_uci (New).csv')

validator.expect_column_values_to_not_be_null('age')

validator.expect_column_values_to_not_be_null('cp')

validator.expect_column_values_to_be_between("trestbps", min_value=0.0 , max_value=200.0)

"""Data Privacy and Security

Data anonymization

1-Randomization:replacing sensitive data with random values
"""

import random
import string

# function generates random strings of length 10 for all columns with object dtype.
def randomize_column(df, cp):
    if cp in df.columns and df[cp].dtype == 'O':  # Check if the specified column is in the dataframe and is of object type
        df[cp] = [''.join(random.choices(string.ascii_letters + string.digits, k=10))
                            for _ in range(len(df))]  # Generate a list of random strings for each row in the specified column
    return df
# Apply the function to the dataframe and a specific column
df_rand1 = randomize_column(df, 'cp')  # Replace 'cp' with the actual column name you want to randomize
print(df_rand1.head())  # Display the first few rows to check the changes

"""2-Generalization:replacing specific values with more general or less precise values."""

# Anonymize the ages by grouping them into age ranges
bins = [0, 20, 30, 40, 50, 60, 70, 80, 90, 100]
labels = ['0.0-19.0', '20.0-29.0', '30.0-39.0', '40.0-49.0', '50.0-59.0', '60.0-69.0', '70.0-79.0', '80.0-89.0', '90.0-100.0']
df['age'] = pd.cut(df['age'], bins=bins, labels=labels)

df

"""3-Masking:replacing sensitive data with a similar but non-sensitive value."""

!pip install faker

!pip install anonymizedf

from anonymizedf.anonymizedf import anonymize

# prepare data for anonymization
an = anonymize(df)

# add masked columns
an.fake_names("dataset")
an.fake_whole_numbers("id")

df
# Patient Identifiers
# Sensitive Personal Information

df

"""4-Perturbation:adding random noise to sensitive data to make it harder to recognize."""

def add_noise(df, column, std=None):
    if std is None:
        std = df[column].std()

    withNoise = df[column].add(np.random.normal(0, std, df.shape[0]))
    copy = df.copy()
    copy[column] = withNoise
    return copy

# Apply perturbation to 'trestbps' with custom standard deviation
perturbed_trestbps = add_noise(df, 'trestbps', std=100)

# Apply perturbation to 'chol'
perturbed_chol = add_noise(df, 'chol',std=100)

# Apply perturbation to 'thalch'
perturbed_thalach = add_noise(df, 'thalch',std=100)
# View the DataFrame to verify changes
print(df[['trestbps', 'chol', 'thalch']])

"""5-Aggregation:combining data from multiple individuals to create a group-level view of the data"""

import pandas as pd
# cholesterol bins and labels
chol_bins = [0, 200, 240, float('inf')]  # Using float('inf') to include all higher values
chol_labels = ['Desirable', 'Borderline high', 'High']

# Aggregate cholesterol data into categories
df['Cholesterol Category'] = pd.cut(df['chol'], bins=chol_bins, labels=chol_labels, right=False)
print(df[['chol', 'Cholesterol Category']].head())


# just a clarrification:
#  Cholesterol level of 233 mg/dL falls into the "Borderline high" category.
#  Cholesterol level of 286 mg/dL is categorized as "High".
#  Cholesterol level of 229 mg/dL is also "Borderline high".
#  Cholesterol level of 250 mg/dL qualifies as "High".
#  Cholesterol level of 204 mg/dL is "Borderline high".

# function to categorize blood pressure
def categorize_blood_pressure(trestbps):
    if trestbps < 120:
        return 'Normal'
    elif 120 <= trestbps < 130:
        return 'Elevated'
    elif 130 <= trestbps < 140:
        return 'Stage 1 Hypertension'
    else:
        return 'Stage 2 Hypertension'

df['BP Category'] = df['trestbps'].apply(categorize_blood_pressure)

df[['trestbps', 'BP Category']].head()

# Define a function to categorize maximum heart rate
def categorize_heart_rate(thalch):
    if thalch < 100:
        return 'Below Average'
    elif 100 <= thalch <= 160:
        return 'Average'
    else:
        return 'Above Average'

df['HR Category'] = df['thalch'].apply(categorize_heart_rate)
df[['thalch', 'HR Category']].head()

"""6-Suppression:removing or omitting sensitive information from a dataset."""

# df_sup = df.drop(columns=[ 'age'])
# df_sup

"""7-Swapping:exchanging or swapping sensitive information between different records"""

# # Swapping data in a specific column (e.g., swapping Age values)
# df_swp = df.copy()
# df_swp['Age_swap'] = df['age'].sample(frac=1).reset_index(drop=True)
# df_swp

"""8-Synthetic:ceating artificial data that resembles the original data but does not contain real information."""

from faker import Faker

# Initialize Faker generator
faker = Faker()
# Generate synthetic data for a new DataFrame
synthetic_data = {
    'age': [faker.name() for _ in range(len(df))],
    'chol': [faker.random_int(18, 80) for _ in range(len(df))],
    'thalch': [faker.random_int(5000,10000) for _ in range(len(df))]
}
df_synthetic = pd.DataFrame(synthetic_data)
print(df_synthetic)
df

"""Data Pseudonymization

1-Scrambling:rearranging or shuffling the characters in a string.
"""

# function to scramble data in a specific column
def scramble_data(data):
    data_list = list(data)
    random.shuffle(data_list)
    return ''.join(data_list)

# Scramble the 'Medical Condition' column
df['Scrambled_dataset'] = df['dataset'].apply(scramble_data)

print(df[['dataset', 'Scrambled_dataset']].head())

"""2-Hashing:converting data into a fixed-size string of bytes using a hash function."""

import hashlib

# Hash the 'restecg' column using a lambda function
df['Hashed_restecg'] = df['restecg'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest()) # Secure Hash Algorithm 256-bit
df

"""4-Encryption:transforming data into a ciphertext"""

from cryptography.fernet import Fernet

# Generate a key
key = Fernet.generate_key()

# Initialize cipher
cipher = Fernet(key)

# Encrypt the 'sex' column using a lambda function
df['Encrypted_sex'] = df['sex'].apply(lambda x: cipher.encrypt(x.encode()).decode())
df

df

# Check for null values in the DataFrame
nulls_present = df.isnull().sum()
print("Null values in each column:")
print(nulls_present)

# Check if there are any null values in the entire DataFrame
any_nulls = df.isnull().any().any()
print("Are there any null values in the DataFrame? ", any_nulls)

# Check for duplicate rows in the DataFrame
duplicates_present = df.duplicated().any()
print("Are there duplicate rows in the DataFrame? ", duplicates_present)

if duplicates_present:
    # Optional: Count the number of duplicate rows
    duplicate_rows = df.duplicated().sum()
    print("Number of duplicate rows: ", duplicate_rows)

"""Access Control

Mandatory access control (MAC)

It is a centralized access control model

Administrators manually classify and label system resources and users based on their risk level and access requirements.

Access to resources is based on the privileges that the user possesses.

OS enforces access rules.

Regular users can’t alter security attributes even for data they’ve created.

It is used in Government organizations, Militaries, and Law enforcement institutions.
"""

class MACPolicy:
    def __init__(self):
        self.access_rules = {}

    def add_access_rule(self, subject, resource, action):
        if resource not in self.access_rules:
            self.access_rules[resource] = {}
        if action not in self.access_rules[resource]:
            self.access_rules[resource][action] = []
        self.access_rules[resource][action].append(subject)

    def check_access(self, subject, resource, action):
        if resource in self.access_rules and action in self.access_rules[resource]:
            return subject in self.access_rules[resource][action]
        return False

mac_policy = MACPolicy()
# Define access rules
mac_policy.add_access_rule("admin", "employee_hr.csv", "read")
mac_policy.add_access_rule("admin", "employee_hr.csv", "write")
mac_policy.add_access_rule("user", "employee_hr.csv", "read")

# Check access before modifying the DataFrame
print(mac_policy.check_access("admin", "employee_hr.csv", "read"))
print(mac_policy.check_access("user", "employee_hr.csv", "write"))

if mac_policy.check_access("admin", "/content/heart_disease_uci (New).csv", "write"):
    # Add a new column 'status' with default value 'active' for all rows
    df['status'] = 'active'
df

# Modify the DataFrame (for example)
if mac_policy.check_access("admin", "employee_hr.csv", "write"):
    df.loc[df['trestbps'] > 50, 'status'] = 'inactive'

# df.to_csv("modified_data.csv", index=False)
df

"""Access Rule Management: Initializes and manages a dictionary of access rules, where each rule defines which subjects (user roles) can perform specific actions (like read or write) on various resources (file names).
Rule Addition: Allows the addition of new access rules, specifying which subjects can perform what actions on which resources, effectively setting the permissions.
Access Verification: Checks if a given subject has the necessary permissions to perform a specified action on a resource, returning True if permitted, otherwise False.






"""